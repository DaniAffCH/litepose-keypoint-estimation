{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lp_coco_utils.lp_getDataset import getDatasetProcessed\n",
    "from lp_training.lp_trainer import train\n",
    "from lp_training.lp_loss import computeLoss\n",
    "from lp_model.lp_litepose import LitePose\n",
    "import lp_config.lp_common_config as cc\n",
    "import torch\n",
    "from lp_training.lp_inference import inference\n",
    "from lp_utils.lp_image_processing import drawHeatmap, drawKeypoints, normalizeImage\n",
    "from lp_testing.lp_evaluation import computeOKS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has to be seen only as an entry that calls wrapper functions, the implentation of those functions can be found in the subdirectories of the repository.   \n",
    "Every hyperparameter can be edited in `src/lp_config`.  \n",
    "`lp_common_config.py` holds the general configurations about the dataset loading, training and test. On the other hand `lp_model_config.py` contrains the parameters that encode the model structure. The current model configs are taken from the Neural Architecture Search performed by the paper authors. I took the small size network due to the computational power available, however better results can be achieved simply by scaling the network size (Good parameters combinations are provided by the paper authors)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken by the [official paper repository](https://github.com/mit-han-lab/litepose):\n",
    "- classes `CocoDataset` and `CocoKeypoints` are partially taken, I added fiftyone support that makes the dataset setup easier and I removed unnecessary code.\n",
    "- I took the code inside `lp_generators.py` and `lp_transforms.py` as well, since they were a `CocoKeypoints` dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The dataset is downloaded by using fiftyone APIs and keypoint heatmaps are created for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/daniaffch/fiftyone/coco-2017/train' if necessary\n",
      "Found annotations at '/home/daniaffch/fiftyone/coco-2017/raw/instances_train2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "loading annotations into memory...\n",
      "Done (t=4.47s)\n",
      "creating index...\n",
      "index created!\n",
      "Downloading split 'validation' to '/home/daniaffch/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/daniaffch/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "loading annotations into memory...\n",
      "Done (t=0.71s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/8015 [00:04<26:42,  4.99it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(cc\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Uni/Neural_Networks/LitePose/litepose-keypoint-estimation/src/lp_training/lp_trainer.py:36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m---> 36\u001b[0m     train_loss \u001b[39m=\u001b[39m trainOneEpoch(model, data_loader, optimizer, i)\n\u001b[1;32m     37\u001b[0m     val_loss_avg \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     38\u001b[0m     val_loss_l \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Uni/Neural_Networks/LitePose/litepose-keypoint-estimation/src/lp_training/lp_trainOne.py:9\u001b[0m, in \u001b[0;36mtrainOneEpoch\u001b[0;34m(model, dataloader, optimizer, epoch, testing)\u001b[0m\n\u001b[1;32m      7\u001b[0m loss_l \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfor\u001b[39;00m images, heatmaps, masks, joints \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(dataloader):\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(config[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Uni/Neural_Networks/LitePose/litepose-keypoint-estimation/src/lp_coco_utils/lp_getDataset.py:344\u001b[0m, in \u001b[0;36mCocoKeypoints.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    341\u001b[0m target_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m--> 344\u001b[0m     img, mask_list, joints_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(\n\u001b[1;32m    345\u001b[0m         img, mask_list, joints_list\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[39mfor\u001b[39;00m scale_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_scales):\n\u001b[1;32m    349\u001b[0m     target_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheatmap_generator[scale_id](joints_list[scale_id])\n",
      "File \u001b[0;32m~/Uni/Neural_Networks/LitePose/litepose-keypoint-estimation/src/lp_coco_utils/lp_transform.py:16\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, image, mask, joints)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, image, mask, joints):\n\u001b[1;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 16\u001b[0m         image, mask, joints \u001b[39m=\u001b[39m t(image, mask, joints)\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m image, mask, joints\n",
      "File \u001b[0;32m~/Uni/Neural_Networks/LitePose/litepose-keypoint-estimation/src/lp_coco_utils/lp_transform.py:167\u001b[0m, in \u001b[0;36mRandomAffineTransform.__call__\u001b[0;34m(self, image, mask, joints)\u001b[0m\n\u001b[1;32m    162\u001b[0m         joints[i][:, :, \u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m joints[i][:, :, \u001b[39m3\u001b[39m] \u001b[39m/\u001b[39m aug_scale\n\u001b[1;32m    164\u001b[0m mat_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_affine_matrix(\n\u001b[1;32m    165\u001b[0m     center, scale, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size), aug_rot\n\u001b[1;32m    166\u001b[0m )[:\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 167\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mwarpAffine(\n\u001b[1;32m    168\u001b[0m     image, mat_input, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size)\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m image, mask, joints\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(cc.config[\"batch_size\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Unfotunately OpenCV method `imshow()` has a well known bug with python notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/daniaffch/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/daniaffch/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "model = LitePose().to(cc.config[\"device\"])\n",
    "model.load_state_dict(torch.load(\"definitiveTag\"))\n",
    "\n",
    "ds = getDatasetProcessed(\"validation\")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "row = next(iter(data_loader))\n",
    "images = row[0].to(cc.config[\"device\"])\n",
    "gthm = row[1]\n",
    "output, keypoints = inference(model, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(output[1][2][:cc.config[\"num_joints\"]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "computeLoss() missing 1 required positional argument: 'gtJoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(n), loss)\n\u001b[1;32m     26\u001b[0m     cv2\u001b[39m.\u001b[39mwaitKey()\n\u001b[0;32m---> 28\u001b[0m computeLoss(output, heatmaps, joints)\n",
      "\u001b[0;31mTypeError\u001b[0m: computeLoss() missing 1 required positional argument: 'gtJoints'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "images = row[0].to(cc.config[\"device\"])\n",
    "gthm = row[1]\n",
    "gtmask = row[2]\n",
    "gtj = row[3]\n",
    "heatmaps = [h.to(cc.config[\"device\"]) for h in gthm]\n",
    "masks = [h.to(cc.config[\"device\"]) for h in gtmask]\n",
    "joints = [j.to(cc.config[\"device\"]) for j in gtj]\n",
    "\n",
    "for n,h in enumerate(output[1][2][:cc.config[\"num_joints\"]]):\n",
    "    h = normalizeImage(h).cpu()\n",
    "    finalHm = cv2.applyColorMap(np.uint8(h), cv2.COLORMAP_JET)\n",
    "\n",
    "    hgt = normalizeImage(gthm[1][2][n])\n",
    "    finalHmgt = cv2.applyColorMap(np.uint8(hgt), cv2.COLORMAP_JET)\n",
    "\n",
    "    loss = (h - hgt)**2 * gtmask[1][2][n]\n",
    "    loss = normalizeImage(loss)\n",
    "    loss = cv2.applyColorMap(np.uint8(loss), cv2.COLORMAP_JET)\n",
    "\n",
    "    cv2.imshow(\"Out\"+str(n), finalHm)\n",
    "    cv2.imshow(\"Gt\"+str(n), finalHmgt)\n",
    "    cv2.imshow(\"Loss\"+str(n), loss)\n",
    "\n",
    "    cv2.waitKey()\n",
    "\n",
    "computeLoss(output, heatmaps, joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jointsHeatmap = output[1][2][:cc.config[\"num_joints\"]]\n",
    "\n",
    "img, finalHm, superimposed = drawHeatmap(images[2], jointsHeatmap)\n",
    "img, gtfinalHm, gtsuperimposed = drawHeatmap(images[2], gthm[1][2])\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.imshow(\"Final heatmap\", finalHm)\n",
    "cv2.imshow(\"Superimposed\", superimposed)\n",
    "\n",
    "cv2.imshow(\"Ground Truth heatmap\", gtfinalHm)\n",
    "cv2.imshow(\"Ground Truth Superimposed\", gtsuperimposed)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = drawKeypoints(images[0], keypoints[0])\n",
    "cv2.imshow(\"Image Keypoints\", img)\n",
    "cv2.waitKey()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
