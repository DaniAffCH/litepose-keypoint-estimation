{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lp_coco_utils.lp_getDataset import getDatasetProcessed\n",
    "from lp_training.lp_trainer import train\n",
    "from lp_model.lp_litepose import LitePose\n",
    "import lp_config.lp_common_config as cc\n",
    "import torch\n",
    "from lp_training.lp_inference import infer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has to be seen only as an entry that calls wrapper functions, the implentation of those functions can be found in the subdirectories of the repository.   \n",
    "Every hyperparameter can be edited in `src/lp_config`.  \n",
    "`lp_common_config.py` holds the general configurations about the dataset loading, training and test. On the other hand `lp_model_config.py` contrains the parameters that encode the model structure. The current model configs are taken from the Neural Architecture Search performed by the paper authors. I took the small size network due to the computational power available, however better results can be achieved simply by scaling the network size (Good parameters combinations are provided by the paper authors)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken by the [official paper repository](https://github.com/mit-han-lab/litepose):\n",
    "- classes `CocoDataset` and `CocoKeypoints` are partially taken, I added fiftyone support that makes the dataset setup easier. In addition I took the code inside `lp_generators.py` as well, since it was a `CocoKeypoints` dependency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The dataset is downloaded by using fiftyone APIs and keypoint heatmaps are created for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/daniaffch/fiftyone/coco-2017/train' if necessary\n",
      "Found annotations at '/home/daniaffch/fiftyone/coco-2017/raw/instances_train2017.json'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(cc\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Uni/Neural_Networks/LitePose/litepose-keypoint-estimation/src/lp_training/lp_trainer.py:13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(batch_size):\n\u001b[0;32m---> 13\u001b[0m     ds \u001b[39m=\u001b[39m getDatasetProcessed(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m     data_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m     16\u001b[0m         ds,\n\u001b[1;32m     17\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     val_ds \u001b[39m=\u001b[39m getDatasetProcessed(\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Uni/Neural_Networks/LitePose/litepose-keypoint-estimation/src/lp_coco_utils/lp_getDataset.py:405\u001b[0m, in \u001b[0;36mgetDatasetProcessed\u001b[0;34m(split, dataset_name, fiftyonepath)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    403\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a dataset split train, validation or test, given \u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 405\u001b[0m foz\u001b[39m.\u001b[39;49mdownload_zoo_dataset(\n\u001b[1;32m    406\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcoco-2017\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    407\u001b[0m     split\u001b[39m=\u001b[39;49msplit\n\u001b[1;32m    408\u001b[0m )\n\u001b[1;32m    410\u001b[0m hm \u001b[39m=\u001b[39m [\n\u001b[1;32m    411\u001b[0m HeatmapGenerator(\n\u001b[1;32m    412\u001b[0m         output_size, config[\u001b[39m\"\u001b[39m\u001b[39mnum_joints\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m2\u001b[39m\n\u001b[1;32m    413\u001b[0m     ) \u001b[39mfor\u001b[39;00m output_size \u001b[39min\u001b[39;00m [\u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m]\n\u001b[1;32m    414\u001b[0m ]\n\u001b[1;32m    416\u001b[0m j \u001b[39m=\u001b[39m [\n\u001b[1;32m    417\u001b[0m     JointsGenerator(\n\u001b[1;32m    418\u001b[0m         \u001b[39m30\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     ) \u001b[39mfor\u001b[39;00m output_size \u001b[39min\u001b[39;00m [\u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m]\n\u001b[1;32m    423\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fiftyone/zoo/datasets/__init__.py:116\u001b[0m, in \u001b[0;36mdownload_zoo_dataset\u001b[0;34m(name, split, splits, dataset_dir, overwrite, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39m\"\"\"Downloads the dataset of the given name from the FiftyOne Dataset Zoo.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[39mAny dataset splits that already exist in the specified directory are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m    -   dataset_dir: the directory containing the dataset\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m zoo_dataset, dataset_dir \u001b[39m=\u001b[39m _parse_dataset_details(\n\u001b[1;32m    114\u001b[0m     name, dataset_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    115\u001b[0m )\n\u001b[0;32m--> 116\u001b[0m \u001b[39mreturn\u001b[39;00m zoo_dataset\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m    117\u001b[0m     dataset_dir\u001b[39m=\u001b[39;49mdataset_dir,\n\u001b[1;32m    118\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m    119\u001b[0m     splits\u001b[39m=\u001b[39;49msplits,\n\u001b[1;32m    120\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    121\u001b[0m     cleanup\u001b[39m=\u001b[39;49mcleanup,\n\u001b[1;32m    122\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fiftyone/zoo/datasets/__init__.py:1016\u001b[0m, in \u001b[0;36mZooDataset.download_and_prepare\u001b[0;34m(self, dataset_dir, split, splits, overwrite, cleanup)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1005\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1006\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDownloading split \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1007\u001b[0m         split,\n\u001b[1;32m   1008\u001b[0m         split_dir,\n\u001b[1;32m   1009\u001b[0m         suffix,\n\u001b[1;32m   1010\u001b[0m     )\n\u001b[1;32m   1012\u001b[0m (\n\u001b[1;32m   1013\u001b[0m     dataset_type,\n\u001b[1;32m   1014\u001b[0m     num_samples,\n\u001b[1;32m   1015\u001b[0m     classes,\n\u001b[0;32m-> 1016\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(split_dir, scratch_dir, split)\n\u001b[1;32m   1018\u001b[0m \u001b[39m# Add split to ZooDatasetInfo\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fiftyone/zoo/datasets/base.py:1146\u001b[0m, in \u001b[0;36mCOCO2017Dataset._download_and_prepare\u001b[0;34m(self, dataset_dir, scratch_dir, split)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dataset_dir, scratch_dir, split):\n\u001b[0;32m-> 1146\u001b[0m     num_samples, classes, downloaded \u001b[39m=\u001b[39m fouc\u001b[39m.\u001b[39;49mdownload_coco_dataset_split(\n\u001b[1;32m   1147\u001b[0m         dataset_dir,\n\u001b[1;32m   1148\u001b[0m         split,\n\u001b[1;32m   1149\u001b[0m         year\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m2017\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1150\u001b[0m         label_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_types,\n\u001b[1;32m   1151\u001b[0m         classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses,\n\u001b[1;32m   1152\u001b[0m         image_ids\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_ids,\n\u001b[1;32m   1153\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_workers,\n\u001b[1;32m   1154\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshuffle,\n\u001b[1;32m   1155\u001b[0m         seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m   1156\u001b[0m         max_samples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_samples,\n\u001b[1;32m   1157\u001b[0m         raw_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_raw_dir(dataset_dir),\n\u001b[1;32m   1158\u001b[0m         scratch_dir\u001b[39m=\u001b[39;49mscratch_dir,\n\u001b[1;32m   1159\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m     dataset_type \u001b[39m=\u001b[39m fot\u001b[39m.\u001b[39mCOCODetectionDataset()\n\u001b[1;32m   1163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fiftyone/utils/coco.py:1594\u001b[0m, in \u001b[0;36mdownload_coco_dataset_split\u001b[0;34m(dataset_dir, split, year, label_types, classes, image_ids, num_workers, shuffle, seed, max_samples, raw_dir, scratch_dir)\u001b[0m\n\u001b[1;32m   1590\u001b[0m unzip_images_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(images_zip_path)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1592\u001b[0m \u001b[39mif\u001b[39;00m classes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m image_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m max_samples \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1593\u001b[0m     \u001b[39m# Full image download\u001b[39;00m\n\u001b[0;32m-> 1594\u001b[0m     num_existing \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(etau\u001b[39m.\u001b[39;49mlist_files(images_dir))\n\u001b[1;32m   1595\u001b[0m     num_download \u001b[39m=\u001b[39m split_size \u001b[39m-\u001b[39m num_existing\n\u001b[1;32m   1596\u001b[0m     \u001b[39mif\u001b[39;00m num_download \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/eta/core/utils.py:3686\u001b[0m, in \u001b[0;36mlist_files\u001b[0;34m(dir_path, abs_paths, recursive, include_hidden_files, sort)\u001b[0m\n\u001b[1;32m   3678\u001b[0m         files\u001b[39m.\u001b[39mextend(\n\u001b[1;32m   3679\u001b[0m             [\n\u001b[1;32m   3680\u001b[0m                 os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrelpath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, f), dir_path)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3683\u001b[0m             ]\n\u001b[1;32m   3684\u001b[0m         )\n\u001b[1;32m   3685\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3686\u001b[0m     files \u001b[39m=\u001b[39m [\n\u001b[1;32m   3687\u001b[0m         f\n\u001b[1;32m   3688\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(dir_path)\n\u001b[1;32m   3689\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dir_path, f))\n\u001b[1;32m   3690\u001b[0m         \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m include_hidden_files)\n\u001b[1;32m   3691\u001b[0m     ]\n\u001b[1;32m   3693\u001b[0m \u001b[39mif\u001b[39;00m sort:\n\u001b[1;32m   3694\u001b[0m     files \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(files)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/eta/core/utils.py:3689\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3678\u001b[0m         files\u001b[39m.\u001b[39mextend(\n\u001b[1;32m   3679\u001b[0m             [\n\u001b[1;32m   3680\u001b[0m                 os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrelpath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, f), dir_path)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3683\u001b[0m             ]\n\u001b[1;32m   3684\u001b[0m         )\n\u001b[1;32m   3685\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3686\u001b[0m     files \u001b[39m=\u001b[39m [\n\u001b[1;32m   3687\u001b[0m         f\n\u001b[1;32m   3688\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(dir_path)\n\u001b[0;32m-> 3689\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misfile(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(dir_path, f))\n\u001b[1;32m   3690\u001b[0m         \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m include_hidden_files)\n\u001b[1;32m   3691\u001b[0m     ]\n\u001b[1;32m   3693\u001b[0m \u001b[39mif\u001b[39;00m sort:\n\u001b[1;32m   3694\u001b[0m     files \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(files)\n",
      "File \u001b[0;32m/usr/lib/python3.10/genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[1;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(cc.config[\"batch_size\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "ToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/daniaffch/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/daniaffch/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "torch.Size([1, 17, 128, 128])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = LitePose().to(cc.config[\"device\"])\n",
    "model.load_state_dict(torch.load(\"definitive1\"))\n",
    "\n",
    "ds = getDatasetProcessed(\"validation\")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "row = next(iter(data_loader))\n",
    "image = row[0].to(cc.config[\"device\"]).unsqueeze(1)\n",
    "heatmaps = row[1]\n",
    "infer(model, image[0], heatmaps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
